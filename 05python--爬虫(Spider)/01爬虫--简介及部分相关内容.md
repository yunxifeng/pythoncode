# 0 爬虫准备工作
- 参考资料
    - python网络数据采集， 图灵工业出版
    - 精通Python爬虫框架Scrapy， 人民邮电出版社
    - [Python3网络爬虫](http://blog.csdn.net/c406495762/article/details/72858983)
    - [Scrapy官方教程](http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/tutorial.html)
- 前提知识
    - url
    - http协议
    - web前端，html, css, js
    - ajax
    - re, xpath
    - xml
# 1.爬虫简介
- 定义: 网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，
        自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。
- 两大特征:
    - 能按照人的意愿下载数据或内容
    - 能自动在网上流窜(自动按照规则跳转,重定向)
- 三大步骤:
    - 1.下载网页
    - 2.提取正确信息
    - 3.根据一定规则自动跳转到其他网页再次执行前两步内容
- 爬虫分类:
    - 通用爬虫
    - 专用爬虫(聚焦爬虫)
- Python网络包简介
    - Python2.x：urllib, urllib2, urllib3, httplib, httplib2, requests
    - Python3.x: urllib, urllib3, httplib2, requests
    - python2: urllib和urllib2配合使用，或者requests
    - Python3：urllib，requests
# 2.urllib包
- 包含模块
    - urllib.request: 打开或读取urls
    - urllib.error: 包含urllib.request产生的常见的错误, 使用try捕捉
    - urllib.parse: 包含解析url的方法
    - urllib.robotparser: 解析robots.txt文件
    - 案例: 01--p01.py
## 关于urllib.request和urllib.parse
 - (1)urllib.request: 打开或读取urls
       - .urlopen(): 请求打开url,并返回HttpRequest格式内容,读取rsp.read()->返回bytes格式,然后再解码decode("utf-8")->返回
         json的Str格式,之后转化为字典格式json.loads(json_data)
       - .Request(): 构建Request实例,将请求所需要的所有信息放入,url,data(dict->Str->bytes),headers等
 - (2)urllib.parse: 包含解析url的方法
       - .urlencode(): 对data(dict格式)进行编码->返回Str格式,需要再编码->bytes格式:encode("utf-8")才能使用
       
- 网页编码问题的解决
    - chardet: 可以自动检测页面文件的编码格式，但是，可能有误
       - chardet.detect(html): 返回dict格式,例: {'encoding': 'UTF-8-SIG', 'confidence': 1.0, 'language': ''}
    - 需要安装: conda install chardet
    - 案例: 01--p02.py
- urllib.request.urlopen(url)的返回对象rsp
    - 返回HttpResponse类型,是HttpResponse的一个实例
        - 案例: 01--p03.py
    - 函数: 
        - rsp.geturl(): 返回请求对象的url
        - rsp.info(): 返回请求对象的meta信息
        - rsp.getcode(): 返回http code
- 访问网络的两种方法
    - 1.get: 利用参数给服务器传递信息
        - 使用urllib.parse对参数(dict格式)进行编码
        - 案例: 01--p03.py
        
    - 2.post: 一般向服务器传递参数使用
        - post会自动加密被传递的信息
        - 如果想要使用post信息,需要使用data参数
        - 使用post,意味着Http的请求头可能需要更改
            - Content-Type: application/x-www.form-urlencode
            - Content-Length: 数据长度
            - 简而言之,一旦更改请求方法,请注意其他请求头部信息相适应
        - urllib.parse.urlencode可以将字符串自动转换成上面的
        - 案例: 01--p04.py
        - 为了设置更多的请求信息,单纯地通过urlopen()函数已经无法满足我们的需求了,
          需要利用request.Request类
        - 案例: 01--p04.py
- 案例总结: 发出请求前: 1.url, 2.data(dict格式)-->[urllib.parse.urlencode(data)编码]-->data(Str格式)-->[encode("utf-8")编码]-->data(Bytes格式)
                        3.headers(dict格式). 全部装进req=urllib.request.Request(url=url, data=data, headers=headers) ,通过urllib.request.urlopen(req)
                        发出,返回HttpResponse的实例rsp    
            返回请求后: 接受到HttpResponse的实例,使用json_data=rsp.read()读取,返回Bytes格式,json_data(Bytes格式)-->[json_data.decode("utf-8")解码]
                       -->json_data(Str格式)-->[json.loads("json_data")解码]-->json_data(dict格式), 正常读取
## 关于urllib.error
- 1.URLError是OSError的一个子类
- URLError产生的原因
    - 没网
    - 服务器连接失败
    - 找不到指定服务器
    - 案例: 01--p05.py

- 2.HttpError是URLError的一个子类
    - 案例: 01--p05.py
- 两者区别:
    - HttpError是对应的HTTP请求的返回码错误,如果返回错误是400以上的,则引发HttpError
    - URLError对应的一般是网络出现问题,包括url问题
- 关系:
    - OSError(父)->URLError(子)->HttpError(孙)
## 关于UserAgent
- 用户代理,简称UA,属于Headers的一部分,服务器通过UA来判断访问者身份
- 常见的UA值: 使用时直接复制粘贴,也可以通过浏览器访问抓包     
    
          - 1.Android
            Mozilla/5.0 (Linux; Android 4.1.1; Nexus 7 Build/JRO03D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Safari/535.19
            Mozilla/5.0 (Linux; U; Android 4.0.4; en-gb; GT-I9300 Build/IMM76D) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30
            Mozilla/5.0 (Linux; U; Android 2.2; en-gb; GT-P1000 Build/FROYO) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1
            
          - 2.Firefox
            Mozilla/5.0 (Windows NT 6.2; WOW64; rv:21.0) Gecko/20100101 Firefox/21.0
            Mozilla/5.0 (Android; Mobile; rv:14.0) Gecko/14.0 Firefox/14.0

          - 3.Google Chrome
            Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/27.0.1453.94 Safari/537.36
            Mozilla/5.0 (Linux; Android 4.0.4; Galaxy Nexus Build/IMM76B) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.133 Mobile Safari/535.19

           - 4.iOS
            Mozilla/5.0 (iPad; CPU OS 5_0 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Version/5.1 Mobile/9A334 Safari/7534.48.3
            Mozilla/5.0 (iPod; U; CPU like Mac OS X; en) AppleWebKit/420.1 (KHTML, like Gecko) Version/3.0 Mobile/3A101a Safari/419.3
- 设置UA两种方式
    - Headers
    - add_header
    - 案例01--p06.py
## ProxyHandler处理器(代理服务器)
- 使用代理IP,是爬虫的常用手段
- 获取代理服务器的地址:
    - 西刺免费代理IP(http://www.xicidaili.com/)
    - 全网代理IP(http://www.goubanjia.com)
- 基本使用步骤:
    - 1.设置代理地址
    - 2.创建ProxyHandler
    - 3.创建Opener
    - 4.安装Opener
- 案例01--p07.py